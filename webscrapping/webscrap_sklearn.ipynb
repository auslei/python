{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import get_news\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "new_path = './news_20191220.pkl'\n",
    "#df = get_news.incrental_load('./news_20191220.pkl')\n",
    "\n",
    "# Incrementally loading data\n",
    "df = pd.read_pickle(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last updated December 20, 2019 19:38:56 AEDT</td>\n",
       "      <td>‘ABSOLUTELY PETRIFIED’: Drug dealer sobs as he...</td>\n",
       "      <td>https://www.news.com.au/national/courts-law/te...</td>\n",
       "      <td>A terrified drug dealer has sobbed and called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last updated December 20, 2019 19:38:57 AEDT</td>\n",
       "      <td>'Do you even care?' Firey blasts ScoMo</td>\n",
       "      <td>https://www.news.com.au/technology/environment...</td>\n",
       "      <td>A fire station officer has posted an emotional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>Roads melting in extreme heat</td>\n",
       "      <td>https://www.news.com.au/technology/environment...</td>\n",
       "      <td>Roads in parts of South Australia are “bleedin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>‘Grossly immoral’: Christians lash Trump</td>\n",
       "      <td>https://www.news.com.au/finance/work/leaders/u...</td>\n",
       "      <td>A major Christian magazine has turned against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>‘This is ridiculous’: Teen’s freakish act</td>\n",
       "      <td>https://www.news.com.au/sport/cricket/big-bash...</td>\n",
       "      <td>The Hobart Hurricanes looked like they were st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      timestamp  \\\n",
       "0  Last updated December 20, 2019 19:38:56 AEDT   \n",
       "1  Last updated December 20, 2019 19:38:57 AEDT   \n",
       "2  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "3  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "4  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "\n",
       "                                            headline  \\\n",
       "0  ‘ABSOLUTELY PETRIFIED’: Drug dealer sobs as he...   \n",
       "1             'Do you even care?' Firey blasts ScoMo   \n",
       "2                      Roads melting in extreme heat   \n",
       "3           ‘Grossly immoral’: Christians lash Trump   \n",
       "4          ‘This is ridiculous’: Teen’s freakish act   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.news.com.au/national/courts-law/te...   \n",
       "1  https://www.news.com.au/technology/environment...   \n",
       "2  https://www.news.com.au/technology/environment...   \n",
       "3  https://www.news.com.au/finance/work/leaders/u...   \n",
       "4  https://www.news.com.au/sport/cricket/big-bash...   \n",
       "\n",
       "                                             content  \n",
       "0  A terrified drug dealer has sobbed and called ...  \n",
       "1  A fire station officer has posted an emotional...  \n",
       "2  Roads in parts of South Australia are “bleedin...  \n",
       "3  A major Christian magazine has turned against ...  \n",
       "4  The Hobart Hurricanes looked like they were st...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stem/Tokenise\n",
    "\n",
    "Tokenise is basically a function converts sentses into \"tokens\" or a list of words. In function below we did **stem** on top of the tokenised words, and removed **stop words** from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\webscrapping\\.env\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "D:\\python\\webscrapping\\.env\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "D:\\python\\webscrapping\\.env\\lib\\site-packages\\nltk\\lm\\vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Counter, Iterable\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "#sample = df.sample(1).content.iloc[0]\n",
    "keep = re.compile('[a-zA-Z]')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def stem_tokenise(corpus):    \n",
    "    return [lemmatizer.lemmatize(ps.stem(w.lower())) for w in word_tokenize(corpus) if re.match(keep, w)]\n",
    "\n",
    "# stem tokenise the stop words as well\n",
    "stop_words = stem_tokenise(\" \".join(list(set(stopwords.words('english')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-6-739772a47b3b>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "  text = re.sub('\\S*@\\S*\\s?', '', text)\n",
      "<ipython-input-6-739772a47b3b>:8: DeprecationWarning: invalid escape sequence \\s\n",
      "  text = re.sub('\\s+', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def cleansing(text):    \n",
    "    # Remove Emails\n",
    "    text = re.sub('\\S*@\\S*\\s?', '', text) \n",
    "\n",
    "    # Remove new line characters\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data = df.content.map(cleansing).map(lambda x: \" \". join(stem_tokenise(x)))\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "#data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "#print(data_lemmatized[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one night a woman heard blind rattl in the room next door hour later her neighbour s bodi wa found in a pool of blood warn graphic how mani australian die by homicid each year and the horrifi way they are lose their live peter dupa arriv at the suprem court.sourc news limit warn graphic thi octob it final look like polic would be abl to have their day in court over the shock nurs home murder of kathleen down have charg one of australia s most deprav serial killer mr down had live at the brunswick lodg nurs home on quiet loyola ave for eight year becom known a the matriarch of the commun despit her age she wa still sprite a person who wa abl to get up and get about a she wa later describ geoff down son of kathleen down who wa stab three time in the neck the onli suspect is peter dupa and the famili is call for an inquest.sourc news limit a is custom a member of staff check on mr down at on new year s eve and found her soundli sleep at anoth resid heard blind rattl and a door open no doubt think it odd that someon wa up at such an hour what the resid heard wa most like the sound of boltcutt snap through the chain lock on mr down bedroom window mayb it wa the flyscreen be slash open nobodi howev report hear mr down scream a she wa stab three time in the neck she wa found at the next morn lie on the floor in a pool of blood she wa the type of ladi who insist on sleep with her bedroom door open detect chief inspector rod collin told the medium shortli after the shock murder the kill ha remain unsolv for year peter dupa who is alreadi serv life sentenc for the stab murder of three woman wa a longtim suspect in the down case he wa formal question in for five hour but deni ani involv in a former lawyer who wa jail for drug traffick and did time with dupa told an inquest that dupa onc confid in him i reckon i m go to end up wear the old sheila down too while at other time allegedli crow they will never get me for that the court heard two call made to the nurs home in novemb and anoth just two hour befor the stab were trace to dupa phone with polic believ evid wa mount up dupa wa charg last year with the brutal kill and wa set to stand trial thi octob howev the former lawyer and key wit for the case wa deem too ill to give evid and prosecutor were forc to drop the case the down slay is onli one of three unsolv murder that polic believ dupa commit in addit to the three he ha been found guilti of the first dupa murder helen mcmahon wa murder while sunbak in a seclud area at rye beach polic believ she wa dupa first kill.sourc suppli dupa first show sign of violent intent toward woman at age visit a neighbour s hous he ask to borrow a sharp knife to cut up some veget he then slash her face neck and hand a she struggl to fight him off he wa place on probat and spent two week under psychiatr care the follow year someon broke into a mortuari and mutil the bodi of two elderli woman a distinct carv wa found on the thigh of one of the bodi a carv that would show up again on one of hi murder victim lead polic to believ dupa may have been involv in dupa wa sentenc to nine year for what the judg call one of the worst rape that could be imagin he wa onli at the time he wa out after just five year le than day after releas he went on a spree sexual assault four woman with vari level of vicious he wa jail again with a five-year minimum a report state bluntli there is littl that can be said in dupa favour hi releas on parol wa a mistake. despit thi in he wa again releas after serv hi minimum sentenc four day later he rape a woman at knifepoint she wa sunbak on a seclud stretch of blairgowri beach when dupa struck the time wa interest to polic just day earlier helen mcmahon had been murder while sunbak in the nearbi sand dune a crime that bore uncanni similar m mcmahon like rye beach for it sen of seclus on februari she did what she had done mani time befor lay her towel out between the sandi dune and sunbak topless at her nake bodi wa found hidden underneath her towel she had been badli beaten thi happen le than from where dupa would rape the sunbak two week later polic question dupa about m mcmahon s murder and he deni ani involv they soon found that dupa wa still imprison dure thi time so they dismiss ani link year later it emerg that dupa wa actual on prereleas the day thi murder occur and live near rye nobodi ha ever been charg for mcmahon s kill polic now believ thi wa dupa first murder the violent death of renita brunton renita brunton wa stab time in a sunburi mall in broad daylight- her friend provid polic with inform that provid a link to dupas.sourc news limit renita brunton wa stab time in broad daylight in a busi mall in sunburi the frenzi attack occur at on novemb the wa in the kitchen of her second-hand cloth store exclus pre-lov cloth nobodi report hear a scream although news soon filter out that she wa seen in the store the previou day have a heat argument with a man who wa never identifi m brunton had onli marri five month earlier so polic follow procedur and both her husband and ex-husband were question both were clear earli on of have ani involv in her death one friend provid polic with inform that provid a link to dupa although thi link wouldn t becom evid until april after he wa charg with the brutal murder of nicol patterson at the time of the brunton murder dupa wa live le than half an hour s drive from the store but more interest to polic he live in woodend the same suburb a renita brunton her friend annett davey told polic m brunton had been hold inform counsel session out the back of her store and wa that day meet a man with a violent sexual histori in dupa use the alia malcolm and organis to meet local psychotherapist nicol patterson after see her classifi ad in a local newspap m patterson wa attempt to move into privat practic and had place the ad in hope of build her client base dupa call her three separ time over a six-week period to inquir about the session and eventu made a appoint one morn at her home her bodi wa discov that even in the front room with stab wound to her chest and spine both of her breast had been cut off and bit of tape were attach to her bodi dupa wa arrest three day later and charg and convict of the murder in the earlier case of renita brunton the murder had also use the ruse of counsel session in order to facilit a meet in a privat locat where she would be vulner dupa ha alway protest hi innoc in the brunton case and provid an alibi to polic but then you put that in the context of the injuri and the type of attack it is and the type of offend he s done in the past those thing have to be weigh up detect sergeant row told channel in after the victorian polic offer a reward of million if it s not peter dupa then who is it three decad of terroris woman peter dupa in court on trial for the murder of mersina halvagi he stab her time dump her bodi in an empti plot three grave from her grandmother.sourc suppli regardless of whether dupa now is ever convict of the three unsolv murder abov he will spend the rest of hi day behind bar with no chanc of parol each time he wa freed from lengthi prison stint he offend again after be freed in he wa again arrest in januari after attempt to rape a woman at knifepoint in a toilet block in septemb he wa free again and kill at least twice the follow year in the earli hour of octob he murder prostitut margaret maher in melbourn a crime he would not be charg with until her bodi wa found under a cardboard box that afternoon by a man who wa collect can roadsid she had been hit in the skull with a cinder block stab in the wrist and choke to death horrif dupa had remov her left breast and stuf it into her mouth justic stephen kay in hand down the verdict said dupa left her bodi by the side of a road in a desol place a a disgust display of loath for the deceas and contempt for her digniti not content with what you had done to her in life you rob her of her digniti in death he said you had over almost three decad terroris woman in thi state. le than a month after maher s kill dupa struck again murder mersina halvagi a she knelt in front of her grandmoth s grave in a greek orthodox cemeteri in melbourn suburb fawkner he stab her time and dump her bodi in an empti plot three grave from her grandmoth nine wit place dupa at the cemeteri that day and he wa a regular at the hotel across the street a with two of hi other victim he slash wildli at her breast but did not remov them it wasn t until that dupa wa final charg for thi murder and found guilti hi third guilti verdict interestingli the former lawyer who wa set to testifi in the nurs home murder thi octob wa instrument in see dupa charg with the graveyard kill tell the court that he confess in jail and even re-enact the murder you seem to be motiv by a deepli entrench pervert and sadist hatr of woman justic elizabeth hollingworth said sentenc dupa to hi third life term a complet contempt for them and their right to live. nathan jolli is a freelanc writer comment to join the convers plea log in dont have an account sign up join the convers you are comment a logout a note about relev advertis we collect inform about the content includ ad you use across thi site and use it to make both advertis and content more relev to you on our network and other site find out more about our polici and your choic includ how to opt-out news pti limit copyright all time aedt gmt power by wordpress.com vip\n"
     ]
    }
   ],
   "source": [
    "print(data.to_list()[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## SKlearn DTM\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Below is an example of sklearn LDA, the problem is it is pretty slow and not as customisable compared to gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  10.002603858444786 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# use own tokeniser and stopwords (note: do the same processing for stop words)\n",
    "#cv = CountVectorizer(stop_words=stop_words, tokenizer=stem_tokenise)\n",
    "\n",
    "cv = CountVectorizer(analyzer='word',       \n",
    "         min_df=10,                        # minimum reqd occurences of a word \n",
    "         stop_words='english',             # remove stop words\n",
    "         lowercase=True,                   # convert all words to lowercase\n",
    "         token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "         # max_features=50000,             # max number of uniq words\n",
    ")\n",
    "\n",
    "\n",
    "# get the dtm (coverting the matrix into dataframe)\n",
    "data_cv = cv.fit_transform(data)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "#pprint(data_dtm)\n",
    "\n",
    "# calculate the sparsity\n",
    "data_dense = data_cv.todense()\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "row = data_dtm.iloc[300]\n",
    "#pprint(row.nlargest(10))\n",
    "#pprint(data.iloc[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## SKlearn Latent Direchlet Allocation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# simple LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter = 10, learning_method = 'online', batch_size = 128, random_state=0)\n",
    "lda_output = lda.fit_transform(data_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood:  -764181.6115431432\n",
      "Perplexity:  861.1281883103927\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': None,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 0,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"log likelihood: \", lda.score(data_cv))\n",
    "print(\"Perplexity: \", lda.perplexity(data_cv))\n",
    "pprint(lda.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Perform a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search_params = {'n_components': [10, 20, 30], 'learning_decay': [0.5, 0.7, 0.9]}\n",
    "lda = LatentDirichletAllocation()\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(data_cv)\n",
    "\n",
    "#best model\n",
    "best_model = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"best model parameters: \", best_model.get_params())\n",
    "result = pd.DataFrame(model.cv_results_)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for x in result.param_learning_decay.unique():\n",
    "    plt.plot(result[result.param_learning_decay==x].param_n_components, result[result.param_learning_decay==x].mean_test_score, label=x)\n",
    "\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Finding Dominate Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = best_model.transform(data_cv)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(df))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(20).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Review Topics Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Visualize the LDA model with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_model, data_cv, cv, mds='tsne')\n",
    "\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "topics = np.argmax(best_model.transform(data_cv), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "webscrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
