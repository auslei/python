{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import get_news\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "new_path = './news_20191220.pkl'\n",
    "#df = get_news.incrental_load('./news_20191220.pkl')\n",
    "\n",
    "# Incrementally loading data\n",
    "df = pd.read_pickle(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last updated December 20, 2019 19:38:56 AEDT</td>\n",
       "      <td>‘ABSOLUTELY PETRIFIED’: Drug dealer sobs as he...</td>\n",
       "      <td>https://www.news.com.au/national/courts-law/te...</td>\n",
       "      <td>A terrified drug dealer has sobbed and called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last updated December 20, 2019 19:38:57 AEDT</td>\n",
       "      <td>'Do you even care?' Firey blasts ScoMo</td>\n",
       "      <td>https://www.news.com.au/technology/environment...</td>\n",
       "      <td>A fire station officer has posted an emotional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>Roads melting in extreme heat</td>\n",
       "      <td>https://www.news.com.au/technology/environment...</td>\n",
       "      <td>Roads in parts of South Australia are “bleedin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>‘Grossly immoral’: Christians lash Trump</td>\n",
       "      <td>https://www.news.com.au/finance/work/leaders/u...</td>\n",
       "      <td>A major Christian magazine has turned against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Last updated December 20, 2019 19:38:58 AEDT</td>\n",
       "      <td>‘This is ridiculous’: Teen’s freakish act</td>\n",
       "      <td>https://www.news.com.au/sport/cricket/big-bash...</td>\n",
       "      <td>The Hobart Hurricanes looked like they were st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      timestamp  \\\n",
       "0  Last updated December 20, 2019 19:38:56 AEDT   \n",
       "1  Last updated December 20, 2019 19:38:57 AEDT   \n",
       "2  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "3  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "4  Last updated December 20, 2019 19:38:58 AEDT   \n",
       "\n",
       "                                            headline  \\\n",
       "0  ‘ABSOLUTELY PETRIFIED’: Drug dealer sobs as he...   \n",
       "1             'Do you even care?' Firey blasts ScoMo   \n",
       "2                      Roads melting in extreme heat   \n",
       "3           ‘Grossly immoral’: Christians lash Trump   \n",
       "4          ‘This is ridiculous’: Teen’s freakish act   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.news.com.au/national/courts-law/te...   \n",
       "1  https://www.news.com.au/technology/environment...   \n",
       "2  https://www.news.com.au/technology/environment...   \n",
       "3  https://www.news.com.au/finance/work/leaders/u...   \n",
       "4  https://www.news.com.au/sport/cricket/big-bash...   \n",
       "\n",
       "                                             content  \n",
       "0  A terrified drug dealer has sobbed and called ...  \n",
       "1  A fire station officer has posted an emotional...  \n",
       "2  Roads in parts of South Australia are “bleedin...  \n",
       "3  A major Christian magazine has turned against ...  \n",
       "4  The Hobart Hurricanes looked like they were st...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Stem/Tokenise\n",
    "\n",
    "Tokenise is basically a function converts sentses into \"tokens\" or a list of words. In function below we did **stem** on top of the tokenised words, and removed **stop words** from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "sample = df.sample(1).content.iloc[0]\n",
    "keep = re.compile('[a-zA-Z]')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_tokenise(corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [ps.stem(w.lower()) for w in word_tokenize(corpus) if w.lower() not in stop_words and re.match(keep, w) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## SKlearn DTM\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  0003587  0003649  000km  007  018  02  022  023  ...  zoned  \\\n",
      "0    0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "1    0    2        2        0      0    0    0   0    0    0  ...      0   \n",
      "2    0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "3    0    1        0        0      0    0    0   0    0    0  ...      0   \n",
      "4    0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "..  ..  ...      ...      ...    ...  ...  ...  ..  ...  ...  ...    ...   \n",
      "66   0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "67   0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "68   0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "69   0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "70   0    0        0        0      0    0    0   0    0    0  ...      0   \n",
      "\n",
      "    zones  zookal  zoom  zqda3na9jp  ztprxqug8u  ºc  ðÿ  über  œairâ  \n",
      "0       0       0     0           0           0   0   0     0      0  \n",
      "1       0       0     0           0           0   0   0     0      0  \n",
      "2       0       0     0           0           0   0   0     0      0  \n",
      "3       0       0     0           0           0   0   0     0      0  \n",
      "4       0       0     0           0           0   0   0     0      0  \n",
      "..    ...     ...   ...         ...         ...  ..  ..   ...    ...  \n",
      "66      0       0     0           0           0   0   0     0      0  \n",
      "67      0       0     0           0           0   0   0     0      0  \n",
      "68      0       0     0           0           0   0   0     0      0  \n",
      "69      0       0     0           0           0   0   0     0      0  \n",
      "70      0       0     0           0           0   0   0     0      0  \n",
      "\n",
      "[142 rows x 11528 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(df.content)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = df.index\n",
    "pprint(data_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Gensim\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "### Dictionary ###\n",
    "**Dictionary** object convert text/sentences to a [list of word] with id's. (https://radimrehurek.com/gensim/corpora/dictionary.html)\n",
    "\n",
    "The object can be incrementally added by calling method *add_documents()*\n",
    "\n",
    "Dictionary can be stored into disk and loaded later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# create a gensim dictory from a pandas series object (created by a column from a dataframe)\n",
    "def create_dictionary(df, column_name, save_to_file=None):\n",
    "    dict = corpora.Dictionary(df[column_name].map(stem_tokenise))\n",
    "    if save_to_file:\n",
    "        dict.save(save_to_file)\n",
    "    return dict\n",
    "\n",
    "#dict = create_dictionary(df, \"content\")\n",
    "dict = corpora.Dictionary.load('my_dict.dict')\n",
    "dict.token2id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bag of Word / TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['absolut', 1.0], ['accept', 1.0], ['acknowledg', 1.0], ['across', 1.0], ['act', 1.0], ['ad', 1.0], ['addict', 1.0], ['advertis', 2.0], ['aedt', 1.0], ['afternoon', 1.0], ['age', 1.0], ['allegedli', 3.0], ['almost', 1.0], ['also', 2.0], ['anoth', 1.0], ['appear', 1.0], ['area', 1.0], ['arrest', 2.0], ['ask', 1.0], ['australia', 10.0], ['away', 1.0], ['back', 1.0], ['background', 1.0], ['bail', 1.0], ['becam', 1.0], ['began', 1.0], ['behind', 1.0], ['believ', 1.0], ['black', 1.0], ['boob', 2.0], ['bought', 1.0], ['bra', 4.0], ['bra.sourc', 1.0], ['brought', 1.0], ['burwood', 1.0], ['buyer', 1.0], ['call', 3.0], ['came', 3.0], ['candace.sutton', 1.0], ['cap', 6.0], ['capsul', 6.0], ['carri', 3.0], ['cell', 1.0], ['centr', 1.0], ['chanc', 1.0], ['chaotic', 1.0], ['childhood', 1.0], ['chill', 1.0], ['choic', 1.0], ['circuz', 1.0], ['clear', 1.0], ['co-offend', 1.0], ['collect', 1.0], ['comfort', 1.0], ['commerci', 1.0], ['community-bas', 1.0], ['condom', 2.0], ['contain', 1.0], ['content', 2.0], ['convers', 1.0], ['cop', 1.0], ['copyright', 1.0], ['corp', 8.0], ['correct', 1.0], ['could', 1.0], ['countri', 1.0], ['court', 6.0], ['courtroom', 1.0], ['cri', 2.0], ['crime', 1.0], ['cuff', 1.0], ['dad', 2.0], ['damian', 2.0], ['day', 1.0], ['deal', 1.0], ['dealer', 5.0], ['dealing.sourc', 1.0], ['death', 1.0], ['decemb', 1.0], ['defcon', 1.0], ['depress', 1.0], ['detail', 2.0], ['deterr', 1.0], ['distraught', 1.0], ['dock', 2.0], ['dog', 1.0], ['down', 1.0], ['downstair', 1.0], ['drag', 2.0], ['dramat', 1.0], ['drug', 19.0], ['dry-ey', 1.0], ['ecstasi', 1.0], ['elig', 1.0], ['embrac', 1.0], ['encrypt', 1.0], ['enter', 1.0], ['epik', 2.0], ['european', 1.0], ['ex-childcar', 1.0], ['excel', 1.0], ['excruci', 1.0], ['face', 2.0], ['facebook', 1.0], ['festiv', 8.0], ['festival.sourc', 1.0], ['find', 1.0], ['first', 1.0], ['found', 1.0], ['free', 1.0], ['freebi', 1.0], ['front', 1.0], ['gambl', 1.0], ['gener', 1.0], ['get', 1.0], ['girl', 1.0], ['glass', 1.0], ['gmt', 1.0], ['go', 1.0], ['got', 1.0], ['grace', 5.0], ['graem', 1.0], ['gram', 1.0], ['graphic', 1.0], ['greed', 1.0], ['guilti', 3.0], ['hand', 2.0], ['handcuf', 1.0], ['handcuff', 1.0], ['handl', 1.0], ['hands.sourc', 1.0], ['harm', 1.0], ['head', 3.0], ['hear', 3.0], ['heard', 4.0], ['hearing.sourc', 1.0], ['hid', 1.0], ['hide', 1.0], ['higher', 1.0], ['hit', 1.0], ['hope', 1.0], ['huge', 1.0], ['hundr', 1.0], ['idea', 1.0], ['impos', 1.0], ['incid', 1.0], ['includ', 2.0], ['indict', 1.0], ['inform', 1.0], ['initi', 1.0], ['instruct', 1.0], ['intercept', 1.0], ['intern', 6.0], ['isol', 1.0], ['j', 1.0], ['ja', 1.0], ['jail', 6.0], ['januari', 1.0], ['judg', 10.0], ['judgment', 1.0], ['julian', 3.0], ['k', 2.0], ['knockout', 1.0], ['lag', 1.0], ['larg', 1.0], ['last', 2.0], ['leant', 1.0], ['learnt', 1.0], ['least', 2.0], ['leav', 1.0], ['left', 1.0], ['lengthi', 1.0], ['less', 1.0], ['limit', 1.0], ['live', 1.0], ['local', 1.0], ['lot', 1.0], ['loudli', 1.0], ['love', 2.0], ['mafia', 1.0], ['make', 1.0], ['male', 1.0], ['man', 1.0], ['man.', 1.0], ['mani', 1.0], ['mark', 2.0], ['martusciello', 18.0], ['maximum', 2.0], ['may', 2.0], ['mdma', 9.0], ['messag', 1.0], ['messeng', 1.0], ['midnight', 1.0], ['milieu.', 1.0], ['minimum', 1.0], ['month', 3.0], ['mother', 5.0], ['move', 2.0], ['mr', 1.0], ['much', 1.0], ['mule', 1.0], ['mule.', 1.0], ['mum', 3.0], ['mum.', 1.0], ['mum.sourc', 1.0], ['music', 7.0], ['need', 2.0], ['negoti', 1.0], ['neither', 1.0], ['network', 1.0], ['news', 9.0], ['news.com.au', 1.0], ['night', 1.0], ['nine', 2.0], ['note', 2.0], ['offenc', 1.0], ['offend', 3.0], ['offic', 3.0], ['olymp', 1.0], ['once.', 1.0], ['one', 2.0], ['opt-out', 1.0], ['outsid', 2.0], ['packag', 3.0], ['parcel', 1.0], ['park', 1.0], ['parol', 1.0], ['part', 2.0], ['paul', 1.0], ['payment', 1.0], ['penalti', 1.0], ['per', 1.0], ['petrifi', 2.0], ['pictur', 2.0], ['pill', 5.0], ['plant', 1.0], ['plead', 3.0], ['plot', 1.0], ['point', 1.0], ['polic', 2.0], ['polici', 1.0], ['poo', 21.0], ['possibl', 1.0], ['power', 1.0], ['precis', 1.0], ['previous', 1.0], ['prison', 8.0], ['profit', 1.0], ['prohibit', 2.0], ['pronounc', 1.0], ['protest', 1.0], ['provid', 1.0], ['pti', 1.0], ['put', 2.0], ['quantiti', 2.0], ['reach', 1.0], ['receiv', 1.0], ['refug', 1.0], ['rehabilit', 1.0], ['relev', 2.0], ['remain', 1.0], ['remark', 1.0], ['remors', 1.0], ['reoffend', 1.0], ['restrict', 1.0], ['retriev', 1.0], ['right', 1.0], ['role', 3.0], ['rubber', 1.0], ['ry', 1.0], ['said', 4.0], ['scale', 1.0], ['scene', 1.0], ['secret', 2.0], ['self', 1.0], ['sell', 1.0], ['sentenc', 7.0], ['serv', 2.0], ['seven', 2.0], ['shawsourc', 2.0], ['site', 2.0], ['six', 2.0], ['small', 1.0], ['smuggl', 6.0], ['sniffer', 1.0], ['so-cal', 1.0], ['sob', 7.0], ['someon', 1.0], ['son', 1.0], ['sorri', 2.0], ['spend', 1.0], ['start', 3.0], ['stash', 1.0], ['stay', 3.0], ['still', 1.0], ['stood', 1.0], ['strong', 2.0], ['success', 1.0], ['suffer', 1.0], ['suppli', 4.0], ['supplier', 1.0], ['surpris', 2.0], ['sydney', 2.0], ['take', 2.0], ['taken', 2.0], ['taker', 1.0], ['target', 1.0], ['term', 1.0], ['terrifi', 2.0], ['terror', 1.0], ['test', 2.0], ['texta', 1.0], ['thousand', 1.0], ['time', 2.0], ['today', 4.0], ['told', 4.0], ['toni', 1.0], ['toward', 1.0], ['tragic', 1.0], ['transact', 1.0], ['trial', 1.0], ['turnbul', 7.0], ['two', 6.0], ['underp', 1.0], ['unemploy', 1.0], ['unfold', 1.0], ['unmark', 1.0], ['use', 4.0], ['vip', 1.0], ['week', 1.0], ['weekend', 1.0], ['well-embed', 1.0], ['wept', 1.0], ['whose', 1.0], ['window', 1.0], ['woman', 3.0], ['wordpress.com', 1.0], ['worker', 1.0], ['would', 4.0], ['wrap', 1.0], ['year', 4.0], ['yet', 1.0]]\n"
     ]
    }
   ],
   "source": [
    "# for each of the document generate bag of the words. Thr reprsentation is (id, count) tuple (only occurring words)\n",
    "def create_bow(data_frame, column_name, dict, save_to_file=None):\n",
    "    my_corpus = [dict.doc2bow(doc, allow_update=True) for doc in data_frame[column_name].map(stem_tokenise)]\n",
    "    if save_to_file:\n",
    "        corpora.MmCorpus.serialize(save_to_file, my_corpus)\n",
    "    return my_corpus\n",
    "\n",
    "#my_corpus = create_bow(df, \"content\", dict, 'bow_corpus.mm')\n",
    "my_corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "for doc in my_corpus:\n",
    "    print([[dict[id], freq] for id, freq in doc])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['absolut', 0.01], ['accept', 0.01], ['acknowledg', 0.02], ['across', 0.0], ['act', 0.01], ['ad', 0.0], ['addict', 0.02], ['advertis', 0.0], ['aedt', 0.0], ['afternoon', 0.02], ['age', 0.01], ['allegedli', 0.05], ['almost', 0.01], ['also', 0.0], ['anoth', 0.01], ['appear', 0.0], ['area', 0.01], ['arrest', 0.03], ['ask', 0.01], ['australia', 0.05], ['away', 0.01], ['back', 0.0], ['background', 0.02], ['bail', 0.02], ['becam', 0.01], ['began', 0.01], ['behind', 0.01], ['believ', 0.01], ['black', 0.01], ['boob', 0.03], ['bought', 0.01], ['bra', 0.08], ['bra.sourc', 0.02], ['brought', 0.01], ['burwood', 0.02], ['buyer', 0.01], ['call', 0.02], ['came', 0.02], ['candace.sutton', 0.02], ['cap', 0.09], ['capsul', 0.15], ['carri', 0.03], ['cell', 0.02], ['centr', 0.01], ['chanc', 0.01], ['chaotic', 0.02], ['childhood', 0.02], ['chill', 0.02], ['choic', 0.0], ['circuz', 0.02], ['clear', 0.01], ['co-offend', 0.02], ['collect', 0.0], ['comfort', 0.01], ['commerci', 0.02], ['community-bas', 0.02], ['condom', 0.05], ['contain', 0.01], ['content', 0.0], ['convers', 0.0], ['cop', 0.02], ['copyright', 0.0], ['corp', 0.08], ['correct', 0.01], ['could', 0.0], ['countri', 0.01], ['court', 0.06], ['courtroom', 0.02], ['cri', 0.03], ['crime', 0.02], ['cuff', 0.02], ['dad', 0.03], ['damian', 0.05], ['day', 0.0], ['deal', 0.01], ['dealer', 0.12], ['dealing.sourc', 0.02], ['death', 0.01], ['decemb', 0.01], ['defcon', 0.02], ['depress', 0.02], ['detail', 0.02], ['deterr', 0.02], ['distraught', 0.02], ['dock', 0.04], ['dog', 0.02], ['down', 0.01], ['downstair', 0.02], ['drag', 0.03], ['dramat', 0.01], ['drug', 0.36], ['dry-ey', 0.02], ['ecstasi', 0.02], ['elig', 0.02], ['embrac', 0.01], ['encrypt', 0.02], ['enter', 0.01], ['epik', 0.05], ['european', 0.02], ['ex-childcar', 0.02], ['excel', 0.02], ['excruci', 0.02], ['face', 0.01], ['facebook', 0.01], ['festiv', 0.1], ['festival.sourc', 0.02], ['find', 0.0], ['first', 0.0], ['found', 0.01], ['free', 0.01], ['freebi', 0.02], ['front', 0.01], ['gambl', 0.02], ['gener', 0.01], ['get', 0.0], ['girl', 0.01], ['glass', 0.02], ['gmt', 0.0], ['go', 0.0], ['got', 0.01], ['grace', 0.12], ['graem', 0.02], ['gram', 0.02], ['graphic', 0.02], ['greed', 0.02], ['guilti', 0.06], ['hand', 0.02], ['handcuf', 0.02], ['handcuff', 0.02], ['handl', 0.01], ['hands.sourc', 0.02], ['harm', 0.01], ['head', 0.02], ['hear', 0.04], ['heard', 0.04], ['hearing.sourc', 0.02], ['hid', 0.02], ['hide', 0.02], ['higher', 0.01], ['hit', 0.01], ['hope', 0.01], ['huge', 0.01], ['hundr', 0.01], ['idea', 0.01], ['impos', 0.02], ['incid', 0.01], ['includ', 0.0], ['indict', 0.02], ['inform', 0.0], ['initi', 0.01], ['instruct', 0.01], ['intercept', 0.02], ['intern', 0.06], ['isol', 0.02], ['j', 0.02], ['ja', 0.02], ['jail', 0.08], ['januari', 0.01], ['judg', 0.16], ['judgment', 0.02], ['julian', 0.05], ['k', 0.05], ['knockout', 0.02], ['lag', 0.02], ['larg', 0.01], ['last', 0.01], ['leant', 0.02], ['learnt', 0.02], ['least', 0.02], ['leav', 0.01], ['left', 0.01], ['lengthi', 0.02], ['less', 0.01], ['limit', 0.0], ['live', 0.0], ['local', 0.01], ['lot', 0.01], ['loudli', 0.02], ['love', 0.01], ['mafia', 0.02], ['make', 0.0], ['male', 0.01], ['man', 0.01], ['man.', 0.02], ['mani', 0.0], ['mark', 0.02], ['martusciello', 0.44], ['maximum', 0.03], ['may', 0.01], ['mdma', 0.22], ['messag', 0.01], ['messeng', 0.02], ['midnight', 0.02], ['milieu.', 0.02], ['minimum', 0.02], ['month', 0.02], ['mother', 0.06], ['move', 0.01], ['mr', 0.01], ['much', 0.0], ['mule', 0.02], ['mule.', 0.02], ['mum', 0.03], ['mum.', 0.02], ['mum.sourc', 0.02], ['music', 0.1], ['need', 0.01], ['negoti', 0.02], ['neither', 0.02], ['network', 0.0], ['news', 0.0], ['news.com.au', 0.01], ['night', 0.01], ['nine', 0.03], ['note', 0.0], ['offenc', 0.01], ['offend', 0.05], ['offic', 0.02], ['olymp', 0.02], ['once.', 0.02], ['one', 0.0], ['opt-out', 0.0], ['outsid', 0.02], ['packag', 0.05], ['parcel', 0.02], ['park', 0.01], ['parol', 0.02], ['part', 0.01], ['paul', 0.01], ['payment', 0.02], ['penalti', 0.02], ['per', 0.01], ['petrifi', 0.05], ['pictur', 0.01], ['pill', 0.1], ['plant', 0.02], ['plead', 0.06], ['plot', 0.02], ['point', 0.01], ['polic', 0.02], ['polici', 0.0], ['poo', 0.51], ['possibl', 0.01], ['power', 0.0], ['precis', 0.02], ['previous', 0.01], ['prison', 0.15], ['profit', 0.02], ['prohibit', 0.05], ['pronounc', 0.02], ['protest', 0.01], ['provid', 0.01], ['pti', 0.0], ['put', 0.01], ['quantiti', 0.03], ['reach', 0.01], ['receiv', 0.01], ['refug', 0.02], ['rehabilit', 0.02], ['relev', 0.0], ['remain', 0.01], ['remark', 0.01], ['remors', 0.02], ['reoffend', 0.02], ['restrict', 0.02], ['retriev', 0.02], ['right', 0.01], ['role', 0.04], ['rubber', 0.02], ['ry', 0.02], ['said', 0.01], ['scale', 0.01], ['scene', 0.01], ['secret', 0.02], ['self', 0.01], ['sell', 0.01], ['sentenc', 0.1], ['serv', 0.02], ['seven', 0.02], ['shawsourc', 0.05], ['site', 0.0], ['six', 0.02], ['small', 0.01], ['smuggl', 0.12], ['sniffer', 0.02], ['so-cal', 0.02], ['sob', 0.15], ['someon', 0.01], ['son', 0.01], ['sorri', 0.03], ['spend', 0.01], ['start', 0.01], ['stash', 0.02], ['stay', 0.02], ['still', 0.01], ['stood', 0.02], ['strong', 0.02], ['success', 0.01], ['suffer', 0.01], ['suppli', 0.01], ['supplier', 0.02], ['surpris', 0.02], ['sydney', 0.01], ['take', 0.01], ['taken', 0.01], ['taker', 0.02], ['target', 0.01], ['term', 0.01], ['terrifi', 0.04], ['terror', 0.02], ['test', 0.02], ['texta', 0.02], ['thousand', 0.01], ['time', 0.0], ['today', 0.03], ['told', 0.02], ['toni', 0.02], ['toward', 0.01], ['tragic', 0.01], ['transact', 0.02], ['trial', 0.01], ['turnbul', 0.17], ['two', 0.02], ['underp', 0.02], ['unemploy', 0.02], ['unfold', 0.01], ['unmark', 0.02], ['use', 0.0], ['vip', 0.0], ['week', 0.01], ['weekend', 0.01], ['well-embed', 0.02], ['wept', 0.02], ['whose', 0.01], ['window', 0.02], ['woman', 0.03], ['wordpress.com', 0.0], ['worker', 0.01], ['would', 0.01], ['wrap', 0.02], ['year', 0.01], ['yet', 0.01]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "import wordcloud as wc\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(my_corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[my_corpus]:\n",
    "    print([[dict[id], np.around(freq, decimals=2)] for id, freq in doc])    \n",
    "    break\n",
    "\n",
    "weights = \n",
    "# get a word cloud going\n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    stopwords=stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "weights = tfidf[my_corpus[0]]\n",
    "\n",
    "weights = [[dict[id], np.around(freq, decimals=2)] for id, freq in weights]\n",
    "\n",
    "wc.generate_from_frequencies(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Download W2V trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('red', 0.8901657462120056),\n",
       " ('black', 0.8648406863212585),\n",
       " ('pink', 0.845291793346405),\n",
       " ('green', 0.8346816301345825),\n",
       " ('yellow', 0.8320707082748413),\n",
       " ('purple', 0.8293111324310303),\n",
       " ('white', 0.8225342035293579),\n",
       " ('orange', 0.8114302158355713),\n",
       " ('bright', 0.799933910369873),\n",
       " ('colored', 0.7876655459403992)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Get information about the model or dataset\n",
    "api.info('glove-wiki-gigaword-50')\n",
    "# {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
    "#  'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
    "#  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
    "#  'file_name': 'glove-wiki-gigaword-50.gz',\n",
    "#  'file_size': 69182535,\n",
    "#  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
    "#  (... truncated...)\n",
    "\n",
    "# Download\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model.most_similar('blue')\n",
    "# [('red', 0.8901656866073608),\n",
    "#  ('black', 0.8648407459259033),\n",
    "#  ('pink', 0.8452916741371155),\n",
    "#  ('green', 0.8346816301345825),\n",
    "#  ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('father', 0.9528983235359192),\n",
       " ('brother', 0.9449328184127808),\n",
       " ('cousin', 0.9256070256233215),\n",
       " ('uncle', 0.9207189679145813),\n",
       " ('nephew', 0.9195291996002197),\n",
       " ('grandson', 0.8999317288398743),\n",
       " ('grandfather', 0.8975841403007507),\n",
       " ('daughter', 0.8961816430091858),\n",
       " ('friend', 0.8672630786895752),\n",
       " ('elder', 0.8604500889778137)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('son')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "webscrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
