{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Random Forest ##\n",
    "\n",
    "*Crowd wisdom*, aggregated predictors usually performs better than single ones. The majority vote method is also know as *emsemble method*. The method works best if predictors are independent (data and method). Random forest is normally emsemble decision trees with bootstraping methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Voting Classifier ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.953030303030303\n",
      "RandomForestClassifier 0.9575757575757575\n",
      "SVC 0.953030303030303\n",
      "VotingClassifier 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "svm_clf = SVC()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "                estimators = [('lr', log_clf), ('svc', svm_clf), ('rf', rnd_clf)],\n",
    "                voting = \"hard\"\n",
    "            )\n",
    "\n",
    "X, y = make_classification(n_samples= 2000, n_classes=2, n_features=20, \n",
    "                           n_redundant = 6, n_clusters_per_class = 2, \n",
    "                           weights = (0.95, 0.05))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## bagging and pasting ##\n",
    "Using same training algorithm for each of predicor, but train in different subset of random data. This is called boostrap aggregating (aka bagging) if sampling is performed with replacement, pasting if without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.956060606060606, 0.9529850746268657)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred), bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Random Pathes and Random Subspace ##\n",
    "\n",
    "Random patches is basically sampling features and data, subspace is sampling features only. Helps with high dimensional data. \n",
    "\n",
    "User hyperparameter: max_features and bootstrap_features\n",
    "\n",
    "For subspace: use bootstrap = False, max_samples = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9575757575757575, 0.9485074626865672)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=500, max_samples=100, bootstrap=True, oob_score=True)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred), rf_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Extra Trees ##\n",
    "\n",
    "*Extremely Randomised* trees. The is for every node, the trees are made random by also using random threshold for each feature, rather than search best possble threshods. This will potentially reduce bias by trading variance, and making treess faster. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ext_clf = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Feature Importance\n",
    "Random forest can also be used to measure the relative imporance of each feature, by measure how much reduction of impurity for each node using weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09726536148450997\n",
      "sepal width (cm) 0.027636121658226105\n",
      "petal length (cm) 0.42961885119732585\n",
      "petal width (cm) 0.44547966565993813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Boosting\n",
    "Boosting involves combine several weak learners. This is to train predictors sequentially, each trying to correct its predecessor.\n",
    "\n",
    "Popular methods are **AdaBoost**(adaptive boosting) and **Gradient Boost**\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S002200009791504X\n",
    "\n",
    "\n",
    "### ADA Boosting ###\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "First base predictor is trained, relative weights of misclassified instances is then increased, the next predictor is trained using updated weights.\n",
    "\n",
    "https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e\n",
    "\n",
    "Start by assigning a weight for each sample, then increase weight for misclassified samples, find the best stump for the next one, and start again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.33333333e-01, 6.00022589e-02, 3.33355372e-01, 1.20397414e-01,\n",
       "        3.33354186e-01, 2.46496772e-04, 3.33379977e-01, 3.24696290e-04,\n",
       "        3.33365707e-01, 3.77301322e-04, 3.33379704e-01, 3.49084999e-04,\n",
       "        3.33368069e-01, 3.77542089e-04, 3.33377933e-01, 3.52106376e-04,\n",
       "        3.33369433e-01, 3.72712090e-04, 3.33376616e-01, 3.54056001e-04,\n",
       "        3.33370427e-01, 3.69265052e-04, 3.33375674e-01, 3.55694330e-04,\n",
       "        3.33371161e-01, 3.66896338e-04, 3.33374996e-01, 3.57017448e-04,\n",
       "        3.33371703e-01, 3.65249478e-04, 3.33374507e-01, 3.58050974e-04,\n",
       "        3.33372101e-01, 3.64091057e-04, 3.33374151e-01, 3.58841485e-04,\n",
       "        3.33372394e-01, 3.63268582e-04, 3.33373893e-01, 3.59437902e-04,\n",
       "        3.33372609e-01, 3.62680377e-04, 3.33373705e-01, 3.59883751e-04,\n",
       "        3.33372767e-01, 3.62257368e-04, 3.33373568e-01, 3.60214928e-04,\n",
       "        3.33372882e-01, 3.61951873e-04]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([0.  , 0.  , 0.56, 0.44]),\n",
       " ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=50)\n",
    "ada_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "s = cross_val_score(ada_clf, iris['data'], iris['target'], cv=6)\n",
    "print(s.mean())\n",
    "\n",
    "ada_clf.estimator_errors_, ada_clf.estimator_weights_, ada_clf.feature_importances_, iris['feature_names']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
